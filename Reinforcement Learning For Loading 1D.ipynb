{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5389a535",
   "metadata": {},
   "source": [
    "# 1 Reward method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af135690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b5d68e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm xây dựng và cập nhật bảng reward\n",
    "def reward_table(bins, items, gamma,x):\n",
    "    n = len(items)\n",
    "    rewards = np.array([[0 for i in range(n)] for j in range(n)],dtype=float)\n",
    "    for i in range(n):\n",
    "        if items[x]>bins[i]:\n",
    "            continue\n",
    "        else:\n",
    "            reward_i = items[x]/bins[i]\n",
    "            tmp_bins_i = bins[i] - items[x]\n",
    "            for j in range(n):\n",
    "                reward_j = 0\n",
    "                if i == j:\n",
    "                    if items[x+1]<=tmp_bins_i:\n",
    "                        reward_j = items[x+1]/tmp_bins_i\n",
    "                else:\n",
    "                    if items[x+1]<=bins[j]:\n",
    "                        reward_j = items[x+1]/bins[j]\n",
    "                rewards[i,j] = reward_i + gamma*reward_j\n",
    "\n",
    "    return rewards\n",
    "\n",
    "# Hàm chọn hành động\n",
    "def choose_action_best_reward(rewards):\n",
    "    return list(np.unravel_index(rewards.argmax(), rewards.shape))\n",
    "\n",
    "# Hàm thực hiện hành động\n",
    "def perform_action(bins,items,action_i,action_j,x):\n",
    "    bins[action_i]-=items[x]\n",
    "    bins[action_j]-=items[x+1]\n",
    "    return bins\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb9c7278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 6, 0, 2, 1, 10, 10, 10]\n"
     ]
    }
   ],
   "source": [
    "# Thử nghiệm\n",
    "items = [2, 5, 4, 7, 1, 3, 8, 9]\n",
    "c = 0\n",
    "bins = [10]*(len(items))\n",
    "\n",
    "while c<len(items)-1:\n",
    "    rewards = reward_table(bins, items, 0.85,c)\n",
    "    best_act = choose_action_best_reward(rewards)\n",
    "    bins = perform_action(bins,items,best_act[0],best_act[1],c)\n",
    "    c+=2\n",
    "print(bins)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47e4f23",
   "metadata": {},
   "source": [
    "# 2 Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "30daa4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def available_action(bins,item): # Lấy ra các action khả thi\n",
    "    avail_act = []\n",
    "    for i in range(len(bins)):\n",
    "        if item<=bins[i]:\n",
    "            avail_act.append(i)\n",
    "    return avail_act\n",
    "    \n",
    "def choose_action(state, bins,item, qTable, epsilon): # Chọn action theo epsilon greedy\n",
    "    actions = available_action(bins,item)\n",
    "    if random.random()<epsilon:\n",
    "        return random.choice(actions)\n",
    "    else:\n",
    "        action = actions[0]\n",
    "        for i in actions:\n",
    "            if qTable[state][i]>qTable[state][i]:\n",
    "                action = i\n",
    "        return action\n",
    "\n",
    "def perform_action(bins, action, item): # Thực hiện action\n",
    "    bins[action] -= item\n",
    "    return bins\n",
    "\n",
    "\n",
    "def reward(bins,item,action): # Tính reward\n",
    "    reward = item/bins[action]\n",
    "    return reward\n",
    "\n",
    "def update_qTable(qTable,state,action,next_state,reward,learning_rate,discount_factor): # Update bảng Q\n",
    "    current_q_Value = qTable[state][action]\n",
    "    next_q_Value = max(qTable[next_state])\n",
    "    new_q_Value = current_q_Value + learning_rate*(reward + discount_factor*next_q_Value - current_q_Value)\n",
    "    qTable[state][action] = new_q_Value\n",
    "    return qTable\n",
    "\n",
    "def q_learning(items,learning_rate,discount_factor,epsilon,episodes,decay_ep,bin_size): # Thực hiện quá trình Q Leaning\n",
    "    n = len(items)\n",
    "    qTable = np.zeros((n+1,n),dtype=float)\n",
    "    for ep in range(episodes):\n",
    "        esl = epsilon\n",
    "        bins = [bin_size]*n\n",
    "        state = 0\n",
    "        for i in range(n):\n",
    "            action = choose_action(state,bins,items[i],qTable,epsilon)\n",
    "            rw = reward(bins,items[i],action)\n",
    "            next_state = action + 1\n",
    "            qTable = update_qTable(qTable,state,action,next_state,rw,learning_rate,discount_factor)\n",
    "            state = next_state\n",
    "    return qTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2bb10896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 3, 2, 1, 10, 10, 10]\n"
     ]
    }
   ],
   "source": [
    "# Thực nghiệm\n",
    "items = [2, 5, 4, 7, 1, 3, 8, 9]\n",
    "bin_size = 10\n",
    "qTable = q_learning(items,0.5,0.5,0.5,10,0.1,bin_size) # Lấy bảng Q table được huấn luyện\n",
    "\n",
    "state = 0\n",
    "bins = [10]*len(items)\n",
    "for item in items: # Thực hiện xếp ống vào thùng theo giá trị Q đã tối ưu trên bảng Q\n",
    "    action = choose_action(state,bins,item,qTable,0) # Đặt tham số epsilon = 0 để chọn action theo Q-table\n",
    "    state = action + 1\n",
    "    bins[action]-=item\n",
    "print(bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e2c55b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
